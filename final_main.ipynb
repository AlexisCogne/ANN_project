{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL MAIN WITH AGENTS FROM 1 TO 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMMON PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "# environment hyperparams\n",
    "n_steps = 500000 # total credit to train the agent\n",
    "n_eval_runs = 10 # how many evaluation runs to do each 20k steps \n",
    "\n",
    "# Aggregate parameters\n",
    "n_seeds = 3 # number of random seeds for the aggregation of plots\n",
    "agents_seeds = [10, 42, 81] # Arbitrary seeds for the agents\n",
    "\n",
    "# agent hyperparams\n",
    "gamma = 0.99  # discount factor\n",
    "ent_coef = 0.01  # coefficient for the entropy bonus (to encourage exploration)\n",
    "actor_lr = 1e-5\n",
    "critic_lr = 1e-3\n",
    "stochastic_reward_probability = 0.9\n",
    "# Note: the actor has a slower learning rate so that the value targets become\n",
    "# more stationary and are theirfore easier to estimate for the critic\n",
    "\n",
    "# DEVICE\n",
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISCRETE CASE 1-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CartPole for the discrete case\n",
    "environment = \"CartPole-v1\"\n",
    "env_eval = gym.make(environment)\n",
    "obs_shape = env_eval.observation_space.shape[0]\n",
    "action_space_dims = env_eval.action_space.n\n",
    "bool_discrete = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGENT 1.A (K = 1 | n = 1) DETERMINISTIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HYPERPARAMETERS SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_agent = \"1A\" # ID of the Agent for the report [1:6]\n",
    "n_envs = 1\n",
    "envs = []\n",
    "for i in range(n_envs):\n",
    "    env = gym.make(environment)\n",
    "    envs.append(env)\n",
    "\n",
    "n_steps_per_update = 1\n",
    "n_updates = n_steps // (n_steps_per_update*n_envs)\n",
    "evaluation_interval = 20000//(n_steps_per_update*n_envs) # evaluate the agent every 20k steps\n",
    "n_evaluations = n_updates // evaluation_interval\n",
    "stochasticity_bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging training variables\n",
    "values,critic_losses,actor_losses,entropies,evaluation_returns_seeds= trainAgent(agents_seeds,n_seeds,envs,env_eval,n_updates,bool_discrete,obs_shape,action_space_dims,device,critic_lr, actor_lr, n_envs,n_steps_per_update, evaluation_interval, n_eval_runs,stochasticity_bool,stochastic_reward_probability,gamma)\n",
    "\n",
    "# Logging variables for each agent\n",
    "values_agent_1A = values.copy()\n",
    "critic_losses_agent_1A = critic_losses.copy()\n",
    "actor_losses_agent_1A = actor_losses.copy()\n",
    "entropies_agent_1A = entropies.copy()\n",
    "evaluation_returns_seeds_agent_1A = evaluation_returns_seeds.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAggregated(values,n_seeds,agents_seeds,critic_losses,actor_losses,entropies,id_agent,n_steps_per_update,n_envs,n_steps,stochasticity_bool,evaluation_returns_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGENT 1.B (K = 1 | n = 1) STOCHASTIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HYPERPARAMETERS SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_agent = \"1B\" # ID of the Agent for the report [1:6]\n",
    "n_envs = 1\n",
    "envs = []\n",
    "for i in range(n_envs):\n",
    "    env = gym.make(environment)\n",
    "    envs.append(env)\n",
    "\n",
    "n_steps_per_update = 1\n",
    "n_updates = n_steps // (n_steps_per_update*n_envs)\n",
    "evaluation_interval = 20000//(n_steps_per_update*n_envs) # evaluate the agent every 20k steps\n",
    "n_evaluations = n_updates // evaluation_interval\n",
    "stochasticity_bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging training variables\n",
    "values,critic_losses,actor_losses,entropies,evaluation_returns_seeds= trainAgent(agents_seeds,n_seeds,envs,env_eval,n_updates,bool_discrete,obs_shape,action_space_dims,device,critic_lr, actor_lr, n_envs,n_steps_per_update, evaluation_interval, n_eval_runs,stochasticity_bool,stochastic_reward_probability,gamma)\n",
    "\n",
    "# Logging variables for each agent\n",
    "values_agent_1B = values.copy()\n",
    "critic_losses_agent_1B = critic_losses.copy()\n",
    "actor_losses_agent_1B = actor_losses.copy()\n",
    "entropies_agent_1B = entropies.copy()\n",
    "evaluation_returns_seeds_agent_1A = evaluation_returns_seeds.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAggregated(values,n_seeds,agents_seeds,critic_losses,actor_losses,entropies,id_agent,n_steps_per_update,n_envs,n_steps,stochasticity_bool,evaluation_returns_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGENT 2 (K = 6 | n = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HYPERPARAMETERS SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_agent = \"2\" # ID of the Agent for the report [1:6]\n",
    "n_envs = 6\n",
    "envs = []\n",
    "for i in range(n_envs):\n",
    "    env = gym.make(environment)\n",
    "    envs.append(env)\n",
    "\n",
    "n_steps_per_update = 1\n",
    "n_updates = n_steps // (n_steps_per_update*n_envs)\n",
    "evaluation_interval = 20000//(n_steps_per_update*n_envs) # evaluate the agent every 20k steps\n",
    "n_evaluations = n_updates // evaluation_interval\n",
    "stochasticity_bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging training variables\n",
    "values,critic_losses,actor_losses,entropies,evaluation_returns_seeds= trainAgent(agents_seeds,n_seeds,envs,env_eval,n_updates,bool_discrete,obs_shape,action_space_dims,device,critic_lr, actor_lr, n_envs,n_steps_per_update, evaluation_interval, n_eval_runs,stochasticity_bool,stochastic_reward_probability,gamma)\n",
    "\n",
    "# Logging variables for each agent\n",
    "values_agent_2 = values.copy()\n",
    "critic_losses_agent_2 = critic_losses.copy()\n",
    "actor_losses_agent_2 = actor_losses.copy()\n",
    "entropies_agent_2 = entropies.copy()\n",
    "evaluation_returns_seeds_agent_2 = evaluation_returns_seeds.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAggregated(values,n_seeds,agents_seeds,critic_losses,actor_losses,entropies,id_agent,n_steps_per_update,n_envs,n_steps,stochasticity_bool,evaluation_returns_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGENT 3 (K = 1 | n = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HYPERPARAMETERS SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_agent = \"3\" # ID of the Agent for the report [1:6]\n",
    "n_envs = 1\n",
    "envs = []\n",
    "for i in range(n_envs):\n",
    "    env = gym.make(environment)\n",
    "    envs.append(env)\n",
    "\n",
    "n_steps_per_update = 6\n",
    "n_updates = n_steps // (n_steps_per_update*n_envs)\n",
    "evaluation_interval = 20000//(n_steps_per_update*n_envs) # evaluate the agent every 20k steps\n",
    "n_evaluations = n_updates // evaluation_interval\n",
    "stochasticity_bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging training variables\n",
    "values,critic_losses,actor_losses,entropies,evaluation_returns_seeds= trainAgent(agents_seeds,n_seeds,envs,env_eval,n_updates,bool_discrete,obs_shape,action_space_dims,device,critic_lr, actor_lr, n_envs,n_steps_per_update, evaluation_interval, n_eval_runs,stochasticity_bool,stochastic_reward_probability,gamma)\n",
    "\n",
    "# Logging variables for each agent\n",
    "values_agent_3 = values.copy()\n",
    "critic_losses_agent_3 = critic_losses.copy()\n",
    "actor_losses_agent_3 = actor_losses.copy()\n",
    "entropies_agent_3 = entropies.copy()\n",
    "evaluation_returns_seeds_agent_3 = evaluation_returns_seeds.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAggregated(values,n_seeds,agents_seeds,critic_losses,actor_losses,entropies,id_agent,n_steps_per_update,n_envs,n_steps,stochasticity_bool,evaluation_returns_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGENT 4 (K = 6 | n = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HYPERPARAMETERS SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_agent = \"4\" # ID of the Agent for the report [1:6]\n",
    "n_envs = 6\n",
    "envs = []\n",
    "for i in range(n_envs):\n",
    "    env = gym.make(environment)\n",
    "    envs.append(env)\n",
    "\n",
    "n_steps_per_update = 6\n",
    "n_updates = n_steps // (n_steps_per_update*n_envs)\n",
    "evaluation_interval = 20000//(n_steps_per_update*n_envs) # evaluate the agent every 20k steps\n",
    "n_evaluations = n_updates // evaluation_interval\n",
    "stochasticity_bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging training variables\n",
    "values,critic_losses,actor_losses,entropies,evaluation_returns_seeds= trainAgent(agents_seeds,n_seeds,envs,env_eval,n_updates,bool_discrete,obs_shape,action_space_dims,device,critic_lr, actor_lr, n_envs,n_steps_per_update, evaluation_interval, n_eval_runs,stochasticity_bool,stochastic_reward_probability,gamma)\n",
    "\n",
    "# Logging variables for each agent\n",
    "values_agent_4 = values.copy()\n",
    "critic_losses_agent_4 = critic_losses.copy()\n",
    "actor_losses_agent_4 = actor_losses.copy()\n",
    "entropies_agent_4 = entropies.copy()\n",
    "evaluation_returns_seeds_agent_4 = evaluation_returns_seeds.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAggregated(values,n_seeds,agents_seeds,critic_losses,actor_losses,entropies,id_agent,n_steps_per_update,n_envs,n_steps,stochasticity_bool,evaluation_returns_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTINUOUS CASE 5-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CartPole for the discrete case\n",
    "environment = \"InvertedPendulum-v4\"\n",
    "env_eval = gym.make(environment)\n",
    "obs_shape = env_eval.observation_space.shape[0]\n",
    "action_space_dims = 1 ## Continuous case ==> 1 dimension of action space: continuous force between [-3, 3] Newton\n",
    "bool_discrete = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGENT 5 (K = 1 | n = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HYPERPARAMETERS SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_agent = \"5\" # ID of the Agent for the report [1:6]\n",
    "n_envs = 1\n",
    "envs = []\n",
    "for i in range(n_envs):\n",
    "    env = gym.make(environment)\n",
    "    envs.append(env)\n",
    "\n",
    "n_steps_per_update = 1\n",
    "n_updates = n_steps // (n_steps_per_update*n_envs)\n",
    "evaluation_interval = 20000//(n_steps_per_update*n_envs) # evaluate the agent every 20k steps\n",
    "n_evaluations = n_updates // evaluation_interval\n",
    "stochasticity_bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging training variables\n",
    "values,critic_losses,actor_losses,entropies,evaluation_returns_seeds= trainAgent(agents_seeds,n_seeds,envs,env_eval,n_updates,bool_discrete,obs_shape,action_space_dims,device,critic_lr, actor_lr, n_envs,n_steps_per_update, evaluation_interval, n_eval_runs,stochasticity_bool,stochastic_reward_probability,gamma)\n",
    "\n",
    "# Logging variables for each agent\n",
    "values_agent_5 = values.copy()\n",
    "critic_losses_agent_5 = critic_losses.copy()\n",
    "actor_losses_agent_5 = actor_losses.copy()\n",
    "entropies_agent_5 = entropies.copy()\n",
    "evaluation_returns_seeds_agent_5 = evaluation_returns_seeds.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAggregated(values,n_seeds,agents_seeds,critic_losses,actor_losses,entropies,id_agent,n_steps_per_update,n_envs,n_steps,stochasticity_bool,evaluation_returns_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGENT 6 (K = 6 | n = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HYPERPARAMETERS SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_agent = \"6\" # ID of the Agent for the report [1:6]\n",
    "n_envs = 6\n",
    "envs = []\n",
    "for i in range(n_envs):\n",
    "    env = gym.make(environment)\n",
    "    envs.append(env)\n",
    "\n",
    "n_steps_per_update = 6\n",
    "n_updates = n_steps // (n_steps_per_update*n_envs)\n",
    "evaluation_interval = 20000//(n_steps_per_update*n_envs) # evaluate the agent every 20k steps\n",
    "n_evaluations = n_updates // evaluation_interval\n",
    "stochasticity_bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging training variables\n",
    "values,critic_losses,actor_losses,entropies,evaluation_returns_seeds= trainAgent(agents_seeds,n_seeds,envs,env_eval,n_updates,bool_discrete,obs_shape,action_space_dims,device,critic_lr, actor_lr, n_envs,n_steps_per_update, evaluation_interval, n_eval_runs,stochasticity_bool,stochastic_reward_probability,gamma)\n",
    "\n",
    "# Logging variables for each agent\n",
    "values_agent_6 = values.copy()\n",
    "critic_losses_agent_6 = critic_losses.copy()\n",
    "actor_losses_agent_6 = actor_losses.copy()\n",
    "entropies_agent_6 = entropies.copy()\n",
    "evaluation_returns_seeds_agent_6 = evaluation_returns_seeds.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAggregated(values,n_seeds,agents_seeds,critic_losses,actor_losses,entropies,id_agent,n_steps_per_update,n_envs,n_steps,stochasticity_bool,evaluation_returns_seeds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
